{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "59ec87b0-f7f6-45b5-b521-e4c7700557e6",
      "metadata": {},
      "source": [
        "# DeID Inference Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048e2749",
      "metadata": {},
      "source": [
        "### Installation and initialise dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f306c3-7ab5-440e-9022-27f4c8ad735f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "import json\n",
        "import glob\n",
        "import os, tabulate, torch\n",
        "from openai import AzureOpenAI\n",
        "import pandas as pd\n",
        "import bitsandbytes, accelerate\n",
        "from typing import Union\n",
        "from evaluate import load\n",
        "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
        "import torch, gc, json, requests\n",
        "from tqdm.notebook import tqdm\n",
        "from medcat.cat import CAT\n",
        "import re\n",
        "import os\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "#Disable parallelism\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "pd.set_option('display.max_rows', 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c5fe68",
      "metadata": {},
      "outputs": [],
      "source": [
        "from config import settings\n",
        "\n",
        "# Top-level configuration variables\n",
        "data_dir = str(settings.DATA_ROOT)\n",
        "AZURE_HDS_URL = AZURE_HDS_URL\n",
        "hyperparameters = {\n",
        "    \"num_shots\": [0],\n",
        "    \"max_new_tokens\": 3000,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.95,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd69725e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from config import settings\n",
        "\n",
        "# Top-level configuration variables\n",
        "# Data directory (from .env via settings)\n",
        "data_dir = str(settings.DATA_ROOT)\n",
        "\n",
        "# Azure Health DeID service URL (editable)\n",
        "AZURE_HDS_URL = AZURE_HDS_URL\n",
        "\n",
        "# Hyperparameters (moved to top)\n",
        "hyperparameters = {\n",
        "    \"num_shots\": [0],\n",
        "    \"max_new_tokens\": 3000,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.95,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ae71f2",
      "metadata": {},
      "source": [
        "## Define Keys and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "327fcf73",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import access_token -- to be included in gitignore\n",
        "import os, sys\n",
        "\n",
        "sys.path.append(\"src\")\n",
        "# Import inference pipeline files\n",
        "import inference_functions as inference\n",
        "\n",
        "# Import dataset processing files and define datadir\n",
        "import results_processing as processing\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "from config import settings\n",
        "\n",
        "AZURE_HDS_BEARER_TOKEN = settings.AZURE_HDS_BEARER_TOKEN or os.getenv(\"AZURE_HDS_BEARER_TOKEN\")\n",
        "if not AZURE_HDS_BEARER_TOKEN:\n",
        "    raise RuntimeError(\"Missing AZURE_HDS_BEARER_TOKEN in environment. Set it in your .env file.\")\n",
        "print(\"Azure HDS token loaded from environment.\")\n",
        "\n",
        "import importlib\n",
        "importlib.reload(inference)\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc0ed174-0df5-473b-8a0d-8a270daff95c",
      "metadata": {},
      "source": [
        "## Load All Data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e04ce7a9-16da-4110-911f-9b2c52bf6fe9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "# A dictionary containing the names of the tasks, and the corresponding directories where the .jsonl fires are stored\n",
        "task_list = {\n",
        "    \"CT_scaphoid\": f\"{data_dir}/CT_scaphoid\",\n",
        "    \"MR_scaphoid\": f\"{data_dir}/MR_scaphoid\",\n",
        "    \"XR_scaphoid\": f\"{data_dir}/XR_scaphoid\",\n",
        "    \"IORD_hist\":f\"{data_dir}/IORD_hist\",\n",
        "    \"IORD_rad\":f\"{data_dir}/IORD_rad\"\n",
        "}\n",
        "\n",
        "# Define a dictionary that will contain loaded dataframes\n",
        "data_list = {}\n",
        "\n",
        "# Load JSONl files in to a df for each task. Return a dict of dataframes, where each df contains all responses for a task\n",
        "for task, dirname in task_list.items():\n",
        "    df = processing.loadall_jsonl_to_df(dirname + \"/*.jsonl\")\n",
        "\n",
        "    #Now flatten all reads in to a single datafram\n",
        "    df = df.sort_values(by=\"id\", ascending=True)\n",
        "    grouped = df.groupby(['id','text'])['label'].apply(list).reset_index()\n",
        "\n",
        "    # Convert list of labelled values into separate columns\n",
        "    labels = grouped['label'].apply(pd.Series).rename(columns = lambda x : 'labelled_' + str(x))\n",
        "\n",
        "    #Reintroduce id and text columns\n",
        "    labels.insert(0, 'id', grouped['id'])\n",
        "    labels.insert(1, 'text', grouped['text'])\n",
        "\n",
        "    data_list[task] = labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d35d77d-ccca-4ec3-9695-21fe55f73f94",
      "metadata": {},
      "source": [
        "## Define Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eba8e37-5fec-48bc-a93b-9ca87dcb6e08",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Prompt is structured: \n",
        "#Task - Please anonymize the following text\n",
        "#Rules - Replace all...word by word - then a list\n",
        "#Examples - for example.\n",
        "#Instructions about output - This is medical...text.\n",
        "\n",
        "instruction = \"\"\"Please anonymize the following text. Replace all of the identifiers with their classification, as specified below. Do this word by word. \n",
        "\n",
        "The following is the list of identifiers and the identifier classification, in the format identifier, [classification]. Each tuple is separated by a semi-colon (;): \n",
        "\n",
        "1. name-doctor, [doctor];\n",
        "2. name-patient, [patient];\n",
        "3. address, [address];\n",
        "4. age if over 89 years old, [age];\n",
        "5. dates, [dates];\n",
        "6. telephone numbers, [phone];\n",
        "7. fax numbers, [fax];\n",
        "8. email, [email];\n",
        "9. social security numbers, [SSN];\n",
        "10. medical record numbers, [MRN];\n",
        "11. NHS number, [NHS number];\n",
        "12. health plan beneficiary numbers, [healthplan];\n",
        "13. account numbers, [account];\n",
        "14. license numbers-GMC, [GMC];\n",
        "15. license numbers-other, [license];\n",
        "16. specimen numbers, [specimen number];\n",
        "17. serial numbers, [serial number];\n",
        "18. vehicle numbers, [vehicle number];\n",
        "19. web-page universal resource locators (URL), [URL];\n",
        "20. internet protocol (IP) numbers, [IP];\n",
        "21. hospital or unit names, [hospital];\n",
        "22. professional titles (for example, dr, or patient professions such as firefighters), [profession];\n",
        "23, organisational or company names, [external healthcare organisation].\n",
        "\n",
        "For example, if you find a doctor's name, such as 'Dr John Doe' replace this with [profession] [doctor] [doctor]. If you find an email address, such as 'jane.doe@nhs.uk', replace this with [email].  \n",
        "\n",
        "This is medical text, please only redact words if they are identifiers. Information such as bones or diseases should be retained. Do not redact categories other than those specified above.\n",
        "\n",
        "Do not output any additional or unnecessary text. Do not offer any opinion or summarisation of the text.\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" In Context learning is a list of tuples, with 'prompt', 'answer' pairs. The prompt generator function will subsequently generate an appropriate thread, e.g. [('raw example 1', 'manually deidentified example 1'), ('raw example 2', 'manually deidentified example 2')]. Please replace these with examples from your own dataset/healthcare context. \"\"\"\n",
        "in_context_examples = ['raw example 1', 'manually deidentified example 1'), ('raw example 2', 'manually deidentified example 2')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc999692",
      "metadata": {},
      "source": [
        "## Experimental Configuration - Check before running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "269be0bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Set true to run sub-samples & False to run a full analysis\n",
        "sub_sample = False\n",
        "sample_size = 10\n",
        "\n",
        "#Set True to save output during processing (FALSE) for test-only runs\n",
        "save_during_execution = True\n",
        "\n",
        "#Specify whether to use the FULL dataset, or for fine-tuned model, to use ONLY the evaluation set (after removal of a training set that is used for fine-tuning)\n",
        "use_test_set_only = True  #MUST set to FALSE if not using Fine-Tuned models\n",
        "fine_tuned_eval_set_location = '{data_dir}/MedCAT_TEST_set.csv'\n",
        "\n",
        "#Output file name\n",
        "output_folder = \"results/\"\n",
        "output_file_name = output_folder+\"2025-02-11 FT-Anoncat New Concepts Added 0.00002 10ep.csv\"\n",
        "\n",
        "#Location of medcat model pack, and any fine tuned versions\n",
        "medcat_model_pack_location = \"/collaborative_development/EHR-DeID-Eval/models/medcat_deid_model_d88707e29606019f.zip\"\n",
        "OUH_fine_tuned_model_location = 'collaborative_development/EHR-DeID-Eval/models/2025-02-07 OUH Anoncat Added Concepts FT 10 epochs 0.00002/model_8056e540f79d9ff6.zip'\n",
        "\n",
        "# Experiment configurations\n",
        "# Define the models, the kwargs to pass to the models, and whether to run on a single GPU or multi-gpu\n",
        "models = pd.DataFrame(\n",
        "    columns=[\"source\", \"model_name\", \"cache_folder\", \"model_kwargs\", \"use_system_prompt\", \"multi_GPU\"],\n",
        "    data=[\n",
        "        [\"medcat\",\"anoncat\", \"\", {}, False, False],\n",
        "        [\"cogstack-modelserve\", \"11 Feb 2025 OUH Fine Tuned AnonCAT 0.00002 10ep new concepts\", \"\", {}, False, False],\n",
        "        [\"hf\", \"google/gemma-7b-it\", \"\", {\"torch_dtype\": torch.bfloat16, \"repetition_penalty\": 2.0}, False, False,],\n",
        "        [\"hf-bert\", \"obi/deid_bert_i2b2\", \"\", {}, False, False],\n",
        "        [\"hf-bert\", \"obi/deid_roberta_i2b2\", \"\", {}, False, False],\n",
        "        [\"hf\", \"microsoft/Phi-3-mini-128k-instruct\", \"\", {}, False, False],\n",
        "        [\"hf\", \"meta-llama/Llama-2-7b-chat-hf\", \"\", {\"torch_dtype\": torch.bfloat16}, False, False],\n",
        "        [\"hf\", \"meta-llama/Llama-2-13b-chat-hf\", \"\", {\"torch_dtype\": torch.bfloat16}, False, True],\n",
        "        [\"hf\", \"meta-llama/Llama-2-70b-chat-hf\", \"\", {\"torch_dtype\": torch.bfloat16}, False, True],\n",
        "        [\"hf\", \"meta-llama/Meta-Llama-3-8B-Instruct\", \"\", {\"torch_dtype\": torch.bfloat16}, False, False],\n",
        "        #[\"vllm\", \"meta-llama/Meta-Llama-3-70B-Instruct\", \"/home/andrewsoltan/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-70B-Instruct/snapshots/5fcb2901844dde3111159f24205b71c25900ffbd\", {\"torch_dtype\": torch.bfloat16}, False, True],\n",
        "        [\"openai\", \"GPT35-turbo-base\", \"{}\", True, False],\n",
        "        [\"openai\", \"GPT-4-0125\", \"\", \"\", True, False],\n",
        "        [\"microsoft_azure_hds\", \"Microsoft Azure De-Identification\", \"\", {}, False, False],\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Define the task name\n",
        "task_name = \"De-Identification\"\n",
        "\n",
        "# Set hyperparameters\n",
        "hyperparameters = {\n",
        "    \"num_shots\": [0],  # A list of the number of shots to include in the prompts/to test\n",
        "    \"max_new_tokens\": 3000,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_k\": 50,\n",
        "    \"top_p\": 0.95,\n",
        "}\n",
        "\n",
        "# Define a list of output metrics\n",
        "metrics_list = [\"rouge_l\", \"bleu\"]\n",
        "\n",
        "# Set up the results dataframe\n",
        "task_results = pd.DataFrame(\n",
        "    columns=[\n",
        "        \"dataset\",\n",
        "        \"id_number_in_dataset\",\n",
        "        \"task_name\",\n",
        "        \"model_name\",\n",
        "        \"model_kwargs\",\n",
        "        \"num_shots\",\n",
        "        \"original_report\",\n",
        "        \"model_output\",\n",
        "    ]\n",
        "    #+ metrics_list\n",
        ")\n",
        "\n",
        "# A flag to prevent the script outputting all outputs dynamically\n",
        "extremely_verbose = False\n",
        "\n",
        "# Set an ID (to be the pseudonymised ID in future iterations)\n",
        "id = 1\n",
        "counter = 1\n",
        "\n",
        "#GC and clear caches prior to starting experiments\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a038262",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" More often than not, the error JSONDecodeError: Expecting value: line 1 column 1 (char 0) means that the Azure DeID bearer token has expired. Check this! \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa42814",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Reconstruct the data_list, which is a dictionary of task-specific data, using the test-set if the TEST SET flag is activated\n",
        "if use_test_set_only:\n",
        "    test_set = pd.read_csv(fine_tuned_eval_set_location)\n",
        "    # Group by 'dataset' column\n",
        "    grouped_by_dataset = test_set.groupby('dataset')\n",
        "\n",
        "    data_list = {name: group for name, group in grouped_by_dataset}\n",
        "\n",
        "# If using cogstack-model serve, now run the service, e.g. with this command in terminal\n",
        "# The below is the location for the fine-tuned model\n",
        "# python3 /collaborative_development/EHR-DeID-Eval/models/cogstack-modelserve/CogStack-ModelServe-master/app/cli/cli.py serve --model-type medcat_deid --model-path '/collaborative_development/EHR-DeID-Eval/models/2024_06_29 OUH Fine Tuned Anoncat/model_704dddef1fb26e21.zip'  --host 127.0.0.1 --port 8000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9fbb961",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Reload the inference functions file\n",
        "import importlib\n",
        "importlib.reload(inference)\n",
        "import re\n",
        "\n",
        "# Cycle through models and run through each model\n",
        "for i, model in models.iterrows():\n",
        "    # To run for multiple examples, add in a second nested loop among training examples. Suggest writing to a JSONL file and then cycling through for each model\n",
        "    print(model[\"model_name\"])\n",
        "\n",
        "    # Empty the cache before loading the model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    \"\"\" For HF models, load the model prior to the looping through items, for speed\"\"\"\n",
        "    if model[\"source\"] == \"hf\":\n",
        "\n",
        "        pipe = inference.hf_load_model(\n",
        "            model[\"model_name\"],\n",
        "            model[\"model_kwargs\"],\n",
        "            multi_gpu=model[\"multi_GPU\"],\n",
        "        )\n",
        "    \n",
        "    if model[\"source\"] == \"hf-bert\":\n",
        "        pipe = inference.hf_load_bert_model(\n",
        "            model[\"model_name\"],\n",
        "            model[\"model_kwargs\"],\n",
        "        )\n",
        "    \n",
        "    \"\"\" For Medcat models, load the model prior to looping for speed\"\"\"\n",
        "    if model[\"source\"] == \"medcat\":\n",
        "        cat = CAT.load_model_pack(medcat_model_pack_location)\n",
        "\n",
        "    \"\"\" For models being served through cogstack-model serve\"\"\"\n",
        "    if model[\"source\"] == \"cogstack-modelserve\":\n",
        "        print (\"*** PLEASE ENSURE THE COGSTACK MODEL-SERVE SERVICE IS RUNNING ON PORT 8000, AND USING THE CORRECT MODEL VERSION *** If this is not done correctly, an error will follow\")\n",
        "\n",
        "    # Cycle through the list of tasks and datasets\n",
        "    for task, data in data_list.items():\n",
        "        \"\"\"For each task dataframe in task_list, run the LLMs on the resultant dataframe\"\"\"\n",
        "        print (task)\n",
        "        \n",
        "        \"\"\" Sample-sample for test runs\"\"\"\n",
        "        if sub_sample:\n",
        "            data = data.sample(sample_size)\n",
        "            print(\"A limit of \"+str(sample_size)+\" is being applied as subsampling is active.\")\n",
        "\n",
        "        ####Â And creates dynamically a list of tuples, called in_context_examples, in format [ (unredacted, redacted) , (unredacted, redacted) ...]\n",
        "\n",
        "        \"\"\"Cycle through each item in each task DF and pass through the model\"\"\"\n",
        "        for index, item in tqdm(data.iterrows(), total=data.shape[0]):\n",
        "            # Set the text in the item as the inference item\n",
        "            item_for_task = item.text\n",
        "\n",
        "            #### Define item id to ensure it can be stored in the output the output\n",
        "            id_of_item_in_dataset = item.id\n",
        "            #print(task)\n",
        "            #print(id_of_item_in_dataset)\n",
        "\n",
        "            \"\"\" Run HF Models\"\"\"\n",
        "            if model[\"source\"] == \"hf\":\n",
        "                \"\"\"Generate appropriate prompts - here with 1 shot learning\"\"\"\n",
        "                prompt_list = inference.generate_prompt_threads(\n",
        "                    instruction,\n",
        "                    in_context_examples,\n",
        "                    item_for_task,\n",
        "                    use_system_prompt=model[\"use_system_prompt\"],\n",
        "                    number_shots=hyperparameters[\"num_shots\"],\n",
        "                )\n",
        "\n",
        "                formatted_prompt_list = inference.hf_generate_prompts(\n",
        "                    pipe,\n",
        "                    prompt_list,\n",
        "                )\n",
        "\n",
        "                # print (formatted_prompt_list[1])\n",
        "\n",
        "                outputs = inference.hf_run_model_using_prompts(\n",
        "                    pipe,\n",
        "                    formatted_prompt_list,\n",
        "                    max_new_tokens=hyperparameters[\"max_new_tokens\"],\n",
        "                    temperature=hyperparameters[\"temperature\"],\n",
        "                    top_k=hyperparameters[\"top_k\"],\n",
        "                    top_p=hyperparameters[\"top_p\"],\n",
        "                )\n",
        "\n",
        "            elif model[\"source\"] == \"hf-bert\":\n",
        "                outputs = [inference.inference_with_bert(pipe, item_for_task)]\n",
        "                #outputs = [inference.reconstruct_bert_output_text(item_for_task,bert_output)]\n",
        "\n",
        "\n",
        "            elif model[\"source\"] == \"openai\":\n",
        "                \"\"\"Run OpenAI Models - here with 1 shot learning\"\"\"\n",
        "                prompt_list = inference.generate_prompt_threads(\n",
        "                    instruction,\n",
        "                    in_context_examples,\n",
        "                    item_for_task,\n",
        "                    use_system_prompt=model[\"use_system_prompt\"],\n",
        "                    number_shots=hyperparameters[\"num_shots\"],\n",
        "                )\n",
        "\n",
        "                outputs = inference.openai_load_client_and_perform_inference(\n",
        "                    os.getenv(\"AZURE_OPENAI_KEY\"),\n",
        "                    os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "                    prompt_list,\n",
        "                    deployment_name=model[\"model_name\"],\n",
        "                    max_new_tokens=hyperparameters[\"max_new_tokens\"],\n",
        "                    temperature=hyperparameters[\"temperature\"],\n",
        "                    top_k=hyperparameters[\"top_k\"],\n",
        "                    top_p=hyperparameters[\"top_p\"],\n",
        "                )\n",
        "\n",
        "            elif model[\"source\"] == \"microsoft_azure_hds\":\n",
        "                \"\"\"Use Microsoft Azure Health Sciences DeID service -- NB there is no option to prompt this service with examples, and hence we only feed in the item for the task\"\"\"\n",
        "                URL = AZURE_HDS_URL\n",
        "                target = \"/api/v1/deid\"\n",
        "                # Return type should be a list with 1 element, only done for the 0-shot version\n",
        "                outputs = [inference.make_request_to_microsoft_deid(URL, target, item_for_task, bearer_token=os.environ[\"AZURE_HDS_BEARER_TOKEN\"])]\n",
        "\n",
        "            elif model[\"source\"] == \"medcat\":\n",
        "                #Get entities within the source text\n",
        "                text = item_for_task\n",
        "                entities = cat.get_entities(text)\n",
        "                #Reconstruct text by iterating through dict backwards, and inserting placeholders for each PIID type, using the pretty_name from the return output\n",
        "                for key, value in reversed(entities['entities'].items()):\n",
        "                    text = inference.reconstruct_anoncat_text(text, value['pretty_name'], value['start'], value['end'])\n",
        "                \n",
        "                #Set the output of the reconstruction as the text\n",
        "                outputs = [text]\n",
        "            \n",
        "            elif model[\"source\"] == \"cogstack-modelserve\":\n",
        "                text = item_for_task\n",
        "                #Next, Make an example POST request to redact text, to test the API\n",
        "                URL = \"http://127.0.0.1:8000/\"\n",
        "                target = \"redact/\"\n",
        "                headers = {}\n",
        "\n",
        "                #Automatically re-run if getting an error 429 from overwhelming\n",
        "                while True:\n",
        "                    returned_result = requests.post(URL + target, headers=headers, data=json.dumps(text))\n",
        "                    if returned_result.status_code == 429:\n",
        "                        print(\"Received 429 Too Many Requests, waiting 2 seconds before retrying...\")\n",
        "                        time.sleep(2)  # Wait for 2 seconds before retrying\n",
        "                    else:\n",
        "                        outputs = [returned_result.text]\n",
        "                        break  # Exit the loop if the status code is not 429\n",
        "\n",
        "            # For the purposes of this demo, the reference text that is being summarised is the last item in the prompt list\n",
        "            referenceText = item_for_task\n",
        "\n",
        "            num_shot_iter = 0\n",
        "\n",
        "            # Return the model summary output and the evaluation scores (in this case, rouge_l), for each of the outputs (i.e. for each of the num_shots values)\n",
        "            for output in outputs:\n",
        "                # Now perform evaluation on the output using the eval function - RougeL has been implemented for this example\n",
        "                #evals = inference.evaluate_output(output, referenceText, metrics_list)\n",
        "\n",
        "                # Create dict per row by merging metrics in with the procedural results in to a single dict\n",
        "                results_dict = {\n",
        "                    \"dataset\": task,\n",
        "                    \"id_number_in_dataset\": id_of_item_in_dataset,\n",
        "                    \"task_name\": task_name,\n",
        "                    \"model_name\": model[\"model_name\"],\n",
        "                    \"model_kwargs\": model[\"model_kwargs\"],\n",
        "                    \"num_shots\": hyperparameters[\"num_shots\"][num_shot_iter],\n",
        "                    \"original_report\": item_for_task,\n",
        "                    \"model_output\": output,\n",
        "                }\n",
        "\n",
        "                # # Append the metrics to the results dict\n",
        "                # list_metrics = [list(metric) for metric in evals.items()]\n",
        "                # results_dict.update(\n",
        "                #     {\n",
        "                #         metric_name: metric[1:]\n",
        "                #         for metric_name, metric in zip(metrics_list, list_metrics)\n",
        "                #     }\n",
        "                # )\n",
        "\n",
        "                # Write results to the dataframe, using a list comprehension to separate out the evals\n",
        "                task_results.loc[len(task_results)] = results_dict\n",
        "\n",
        "                # Increment counter\n",
        "                num_shot_iter = num_shot_iter + 1\n",
        "\n",
        "                \"\"\" Set extremely_verbose to False to spare your screen \"\"\"\n",
        "                if extremely_verbose:\n",
        "                    print(output)\n",
        "                    print(\"\\n\")\n",
        "                    print(evals)\n",
        "                    print(\"\\n\")\n",
        "        \n",
        "        #If for saving during execution, output to file\n",
        "        if (save_during_execution):\n",
        "            print (\"Saving output\")\n",
        "            task_results.to_csv(output_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0674e1bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the results!\n",
        "# To view the results in true order, do a multi-level sort for [\"dataset\",\"id_number_in_dataset\"]\n",
        "task_results.sort_values(by=[\"dataset\",\"id_number_in_dataset\"]).head(50)\n",
        "task_results.to_csv(\"output.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d75842",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa11586d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_request_to_microsoft_deid (URL, target, text, bearer_token=os.environ[\"AZURE_HDS_BEARER_TOKEN\"]):\n",
        "    \n",
        "    headers = {\n",
        "        \"Authorization\": \"Bearer \" + bearer_token,\n",
        "        \"Content-Type\" : \"application/json\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "            \"DataType\":\"Plaintext\",\n",
        "            \"Operation\":\"Redact\",\n",
        "            \"InputText\": text\n",
        "    }\n",
        "\n",
        "    #Make the request to the Microsoft DeID Service\n",
        "    #If there is an error, a likely outcome is the bearer token is incorrect\n",
        "    #try:\n",
        "    response = requests.post(URL+target, headers=headers, json=payload)\n",
        "\n",
        "    # Status Code\n",
        "    print(\"Status Code:\", response.status_code)\n",
        "\n",
        "    # Response Text\n",
        "    print(\"Response Text:\", response.text)\n",
        "\n",
        "    # Response JSON (if available)\n",
        "    try:\n",
        "        print(\"JSON Response:\", response.json())\n",
        "    except ValueError:\n",
        "        print(\"No JSON in response.\")\n",
        "\n",
        "    # Response Headers\n",
        "    print(\"Headers:\", response.headers)\n",
        "\n",
        "    # Response Content (raw bytes)\n",
        "    print(\"Content (bytes):\", response.content)\n",
        "\n",
        "    # URL (final URL after redirections, if any)\n",
        "    print(\"Final URL:\", response.url)\n",
        "\n",
        "    # Reason for the response status\n",
        "    print(\"Reason:\", response.reason)\n",
        "\n",
        "    # Encoding of the response\n",
        "    print(\"Encoding:\", response.encoding)\n",
        "\n",
        "    # Cookies (if any)\n",
        "    print(\"Cookies:\", response.cookies)\n",
        "\n",
        "    # Elapsed time for the request\n",
        "    print(\"Elapsed Time:\", response.elapsed)\n",
        "\n",
        "    #output=anonreturn['outputText']  \n",
        "    print (anonreturn)\n",
        "    # except:\n",
        "    #     #print (\"Error, is your bearer token correct?\")\n",
        "    #     output = \"Error, is your bearer token valid?\"\n",
        "    #     #print (anonreturn)\n",
        "\n",
        "    #Now return the reports and add to the series\n",
        "    #return (output)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
