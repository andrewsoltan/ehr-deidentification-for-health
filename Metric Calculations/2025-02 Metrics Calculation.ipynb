{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to calculate all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import difflib\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tqdm import tqdm\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load annotation files\n",
    "def loadjsonl_to_df(jsonl_file):\n",
    "    jsonl_data = []\n",
    "    with open(jsonl_file, 'r') as file:\n",
    "        for line in file:\n",
    "            jsonl_data.append(json.loads(line))\n",
    "            \n",
    "        df = pd.DataFrame(jsonl_data)\n",
    "        return df\n",
    "\n",
    "#Process reports to remove extra spaces/new lines \n",
    "def preprocess_text(text):\n",
    "    #remove newline characters (\\n) and paragraph markers (\\n\\n or /)\n",
    "    text = re.sub(r'\\n', ' ', text) #remove newline characters and replace with a single space, with the exception of dates\n",
    "    return text \n",
    "\n",
    "def apply_preprocessing(row):\n",
    "    return preprocess_text(row['text'])\n",
    "\n",
    "#Redact text using doccano output\n",
    "def redact_text(row):\n",
    "    text = preprocess_text(row['text']) #extract text from each row, using the processed text\n",
    "    manual_output = row['manual_output'] #extract manual_output for each row\n",
    "    redacted_text = text #initialise redacted text as the original text\n",
    "    offset = 0 #initialise offset to keep track fo cumulative position changes from preprocessing \n",
    "\n",
    "    #iterate over each sublist in manual_output\n",
    "    for annotation in manual_output:\n",
    "        start_token, end_token, string = annotation #extract the start, end tokens and string for each sublist\n",
    "        if string != 'time':\n",
    "            update_start = start_token + offset\n",
    "            update_end = end_token + offset\n",
    "            redacted_text = redacted_text[:update_start] + '[' + string + ']' + redacted_text[update_end:] #replaced the text from start-end token with string and square brackets\n",
    "            len_diff = len(string) - (end_token - start_token)\n",
    "            offset += len_diff + 2 #calculate and add offset, and add +2 to account for the addition of the square brackets\n",
    "\n",
    "    redacted_text = re.sub(r'(?<=\\s)\\d{4}(?=\\s|$|[.,!?();:])', '[date]', redacted_text) # replace 4-digit numbers preceded by a space with '[date]'\n",
    "\n",
    "    return redacted_text\n",
    "\n",
    "#Combine dataframes\n",
    "#df1 and df2 refer to the df to be merged\n",
    "#merge_criteria is the column to merge the df on\n",
    "#drop columns before/after merge is a list of columns to remove if required (put in [])\n",
    "\n",
    "def combine_df(df1, df2, merge_criteria, drop_col_before_merge=None, drop_col_after_merge=None):\n",
    "    \n",
    "    if drop_col_before_merge:\n",
    "        df1 = df1.drop(columns=drop_col_before_merge, errors='ignore')\n",
    "        df2 = df2.drop(columns=drop_col_before_merge, errors='ignore')\n",
    "    \n",
    "    df_comb = pd.merge(df1, df2, on=merge_criteria)\n",
    "    \n",
    "    if drop_col_after_merge:\n",
    "        df_comb = df_comb.drop(columns=drop_col_after_merge, errors='ignore')\n",
    "    \n",
    "    return df_comb\n",
    "\n",
    "#extract lists of redacted words\n",
    "def extract_redacted_words(row, column):\n",
    "    text_words = row['text'].split()\n",
    "    column_words = row[column].split()\n",
    "    redacted_words = [word for word in text_words if word not in column_words]\n",
    "    return redacted_words\n",
    "\n",
    "\n",
    "#Find the first shared word and update 'model' column\n",
    "def update_model(model_text, text_text):\n",
    "    model_words = model_text.split() #split text into lists of words\n",
    "    text_words = text_text.split()\n",
    "    shared_word = next((word for word in model_words if word in text_words), None) #find first shared word between model/text\n",
    "\n",
    "    if shared_word:\n",
    "        index = model_text.find(shared_word) #find index of shared word in model text\n",
    "        return model_text[index:].strip() #keep substring of model text starting from the shared word\n",
    "    return model_text #if no shared word is found, return the original model text\n",
    "\n",
    "def calculate_metrics(model_redaction, manual_redaction):\n",
    "    tp = fp = fn = 0 #initialise tp, fp, fn counts as 0\n",
    "\n",
    "    tp_words = []\n",
    "    fp_words = []\n",
    "    fn_words =[]\n",
    "\n",
    "    #copy each of model_redaction and manual_redaction for modification \n",
    "    model_redaction_processed = model_redaction.copy()\n",
    "    manual_redaction_processed = manual_redaction.copy()\n",
    "\n",
    "    #calculate tp\n",
    "    for word in manual_redaction: #for each word in manual_redaction\n",
    "        while word in model_redaction_processed: #if the word is also in manual_redaction\n",
    "            tp += 1 #increment one to tp\n",
    "            tp_words.append(word)\n",
    "\n",
    "            #remove word from both lists to prevent double counting\n",
    "            model_redaction_processed.remove(word) #modify the copies only\n",
    "            manual_redaction_processed.remove(word)\n",
    "\n",
    "    #calculate fn\n",
    "    for word in manual_redaction_processed:\n",
    "        if word not in model_redaction_processed:\n",
    "            fn += 1 #increment one to fn\n",
    "            fn_words.append(word)\n",
    "    \n",
    "    #calculate fp\n",
    "    for word in model_redaction_processed:\n",
    "        if word not in manual_redaction:\n",
    "            fp += 1 #increment one to fp\n",
    "            fp_words.append(word)\n",
    "\n",
    "    return tp, fp, fn, tp_words, fp_words, fn_words\n",
    "\n",
    "def output_metrics(row):\n",
    "    return calculate_metrics(row['model_redaction'], row['manual_redaction'])\n",
    "\n",
    "# Function to calculate metrics by row\n",
    "def calculate_metrics_by_row(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "def report_metrics_with_CIs (row, comb, by_dataset=False):\n",
    "    n_bootstrap = 10000\n",
    "    confidence_level = 95\n",
    "    alpha = (100 - confidence_level) / 2\n",
    "\n",
    "    # Disable iteration through each row\n",
    "    #for index, row in grouped_comb.iterrows():\n",
    "    model_name = row['model_name'][0]\n",
    "    num_shots = row['num_shots'][0]\n",
    "    tp_count = row['tp'][0]\n",
    "    fp_count = row['fp'][0]\n",
    "    fn_count = row['fn'][0]\n",
    "    \n",
    "    # Calculate observed precision, recall, and F1\n",
    "    precision, recall, f1 = calculate_metrics_by_row(tp_count, fp_count, fn_count)\n",
    "    \n",
    "    # Bootstrap lists\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    bleu_scores_model_ref = []\n",
    "    \n",
    "    # Bootstrap sampling\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample within the current group only\n",
    "        sample = comb[(comb['model_name'] == model_name) & (comb['num_shots'] == num_shots)].sample(frac=1, replace=True)\n",
    "        sample_tp = sample['tp'].sum()\n",
    "        sample_fp = sample['fp'].sum()\n",
    "        sample_fn = sample['fn'].sum()\n",
    "        \n",
    "        # Call function to calculate metrics\n",
    "        sample_precision, sample_recall, sample_f1 = calculate_metrics_by_row(sample_tp, sample_fp, sample_fn)\n",
    "        \n",
    "        precisions.append(sample_precision)\n",
    "        recalls.append(sample_recall)\n",
    "        f1_scores.append(sample_f1)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    precision_ci = np.percentile(precisions, [alpha, 100 - alpha])\n",
    "    recall_ci = np.percentile(recalls, [alpha, 100 - alpha])\n",
    "    f1_ci = np.percentile(f1_scores, [alpha, 100 - alpha])\n",
    "    \n",
    "    # Print results, including if by dataset\n",
    "    if by_dataset:\n",
    "        print(f\"Results for model '{model_name}' with num_shots '{num_shots}' on dataset '{row['dataset'][0]}':\")\n",
    "    else:\n",
    "        print(f\"Results for model '{model_name}' with num_shots '{num_shots}':\")\n",
    "\n",
    "    # print(f\"TP count: {tp_count}\")\n",
    "    # print(f\"FP count: {fp_count}\")\n",
    "    # print(f\"FN count: {fn_count}\")\n",
    "    print(f\"Precision: {precision:.3f} (95% CI: {precision_ci[0]:.3f}, {precision_ci[1]:.3f})\")\n",
    "    precision = (f\"{precision:.3f} (95% CI: {precision_ci[0]:.3f}, {precision_ci[1]:.3f})\")\n",
    "    print(f\"Recall: {recall:.3f} (95% CI: {recall_ci[0]:.3f}, {recall_ci[1]:.3f})\")\n",
    "    recall = (f\"{recall:.3f} (95% CI: {recall_ci[0]:.3f}, {recall_ci[1]:.3f})\")\n",
    "    print(f\"F1-score: {f1:.3f} (95% CI: {f1_ci[0]:.3f}, {f1_ci[1]:.3f})\")\n",
    "    f1= (f\"{f1:.3f} (95% CI: {f1_ci[0]:.3f}, {f1_ci[1]:.3f})\")\n",
    "    print()  # Add an empty line for spacing\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words deemed not applicable as professional titles - from RQ v1 code\n",
    "def remove_excluded_words(column):\n",
    "    #List of professional labels\n",
    "    not_applicable= ['consultant:', 'consultant', '(consultant', '\"consultant,\"', 'consultant.', 'consultant)', '(consultant)', 'report/consultant', 'cons', '\"consultant,\"', \"consultant,\", 'consultant:',\n",
    "    'pathologist:', 'pathologist', 'dermatopathologists.', 'neuropathologist', '\"pathologist,\"', 'haematopathologist', 'pathologist)', 'pathologists', 'haematopathologist.', '\"pathologist,\"', '\"pathologist,\"',\n",
    "    'bms.', 'bms', '(bms).', 'bms.dictated', 'bms.one', 'bms.-------------------x--------------------', '(sonographer)',\n",
    "    'pathology', '\"pathology,\"', 'neuropath',\n",
    "    'cellular', '(cellular', \n",
    "    'msk', '(msk', 'reporting', 'musculoskeletal',\n",
    "    'registrar:', '\"registrar\"', 'registrar)', 'registrar', 'registrar.', 'spr', 'fellow.', 'fellow)', 'fellow', 'fellow).', '\"fellow,\"', '\"registrar,\"', '(fellow)', '(fellow)',\n",
    "    'radiologist', 'radiologist)', '(radiologist)', 'radiologist.', '\"neuroradiologist\"', '\"radiologist\"', '(radiologist', 'neuroradiologist', 'neuroradiologist)', 'radiologists', 'radiologist:', 'neuroradiologist.', '\"radiologist),\"', 'radiologist;',\n",
    "    'prof', 'professor', 'raioogist', 'orthopod',\n",
    "    'gmc', 'gmc:', 'ltd', 'nan', 'ref:', 'resident', 'as', 'of', 'senior', \n",
    "    'clinicians', 'clinician' 'clinician:', '(clinical', 'clinic', 'clinic.', 'hand', '(ss',\n",
    "    'team.',\n",
    "    'speciality', 'specialty', 'specialist', '(specialist', '\"neuropath,\"',\n",
    "    'al', '(general', '\"radiologist),\"', 'suggestive', 'supervised', 'surg', 'pathol',\n",
    "    'scientist', 'scientist.', '\"scientist\"\"\"', '\"registrar,\"', 'registered', 'dated:',\n",
    "    'received', '\"neuroradiologist,\"', 'sonographer)', 'physio', '\"pathologist,\"', '(dermatopathology)', 'agreed',\n",
    "    'senior', '15:16', 'regi?tered', 'new', 'clinic',\n",
    "    'number:', 'st4', 'st2', 'st5', 'st3', 'st1', '(haematopathologist)', 'urological',\n",
    "    'principal', '\"pathology,\"', '\"sonographer),\"', 'reported', 'had', 'cross-sectional', 'cxr', 'consensus',\n",
    "    '(trainee', 'trainee)', 'trainee', 'lead', 'mdt', 'lead)',\n",
    "    'for', 'inform', 'formal', '\"radiologist),\"', '\"registrar,\"', 'anaesthetist', 'classification.',\n",
    "    'is', '-', 'at', 'pre-registration', 'onwards', 'miu', ')', '(senior',\n",
    "    'swifft', 'post', 'cct', 'radiograph', 'year', '\"neuroradiologist,\"', '\"radiologist,\"', 'feels', '\"radiologist),\"', \"(radiographer)\",\n",
    "    'surgeon', 'radiology', '(radiology', 'omfs', 'ortho', 'sho', 'doctor,', 'shows', '\"radiologist),\"', 'radiology.', 'a&e', 'gynaecologist', '\"neuroradiologist,\"', 'neurology', '\"radiologist,\"', 'trauma', 'likely',\n",
    "    'radiographer', 'neurointerventional', '\"neurologist,\"', '\"spr,\"', 'gp', 'neurosurgery', 'sonographer', 'radiographers', '(sonographer).', '\"neuroradiologist,\"', '\"sonographer),\"', 'radiographer.']\n",
    "\n",
    "    return (column.apply(lambda word_list: [word for word in word_list if word.lower() not in not_applicable]))\n",
    "\n",
    "#Remove professional titles for models that don't do this\n",
    "def exclude_professional_details(column):\n",
    "    professional_titles=['dr', 'dr.', 'prof', 'professor', 'prof.']\n",
    "    \n",
    "    return (column.apply(lambda word_list: [word for word in word_list if word.lower() not in professional_titles]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to calculate bleu\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(reference_texts, candidate_texts):\n",
    "    return corpus_bleu([[tokenize(ref)] for ref in reference_texts], [tokenize(candidate) for candidate in candidate_texts])\n",
    "\n",
    "# Function to calculate bootstrap confidence intervals for BLEU scores\n",
    "def bootstrap_bleu(reference, hypothesis, num_samples=100, ci=95):\n",
    "    bleu_scores = []\n",
    "    n = len(reference)\n",
    "    for _ in range(num_samples):\n",
    "        indices = np.random.choice(range(n), size=n, replace=True)\n",
    "        resampled_reference = [reference[i] for i in indices]\n",
    "        resampled_hypothesis = [hypothesis[i] for i in indices]\n",
    "        bleu_score = corpus_bleu([[tokenize(ref)] for ref in resampled_reference], \n",
    "                                 [tokenize(hyp) for hyp in resampled_hypothesis])\n",
    "        bleu_scores.append(bleu_score)\n",
    "    lower_bound = np.percentile(bleu_scores, (100 - ci) / 2)\n",
    "    upper_bound = np.percentile(bleu_scores, 100 - (100 - ci) / 2)\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "#Functions to calculate Levenshtein distance\n",
    "def safe_levenshtein(str1, str2):\n",
    "    if pd.isna(str1) or pd.isna(str2):\n",
    "        return float('nan')\n",
    "    return Levenshtein.distance(str1, str2)\n",
    "\n",
    "# Function to calculate bootstrap confidence interval\n",
    "def bootstrap_ci_levenshtein(data, num_samples=100, ci=95):\n",
    "    bootstrap_means = []\n",
    "    for _ in range(num_samples):\n",
    "        sample = np.random.choice(data.dropna(), size=len(data.dropna()), replace=True)\n",
    "        bootstrap_means.append(np.mean(sample))\n",
    "    lower_bound = np.percentile(bootstrap_means, (100 - ci) / 2)\n",
    "    upper_bound = np.percentile(bootstrap_means, 100 - (100 - ci) / 2)\n",
    "    return lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, load the annotated files\n",
    "#list all jsonl files in a specified directory\n",
    "#Note that the format of the df will be: id (unique study id), text (reference text), manual_output (labelling) and dataset (where the data has originated from)\n",
    "file_list = glob.glob('/collaborative_development/EHR-DeID-Eval/data_2024_03_08/annotations/*.jsonl')\n",
    "\n",
    "#initialise empty list to store all doccano df\n",
    "dfs = []\n",
    "\n",
    "#iterate over each doccano file and append to the combined df\n",
    "for file in file_list:\n",
    "    df = pd.read_json(file, lines=True)\n",
    "\n",
    "    #add new column called 'dataset' that contains the dataset name for reference\n",
    "    filename = os.path.basename(file)  \n",
    "    dataset = os.path.splitext(filename)[0]\n",
    "    df['dataset'] = dataset\n",
    "    dfs.append(df)\n",
    "\n",
    "#concatenate all df into one\n",
    "full_ann1 = pd.concat(dfs, ignore_index=True)\n",
    "full_ann = full_ann1.drop(['Comments', 'study_id'], axis=1)\n",
    "full_ann.rename(columns={'label': 'manual_output'}, inplace=True)\n",
    "\n",
    "#display df for sense check (comment in or out depending on needs)\n",
    "#pd.set_option('max_colwidth', None) #set max colwidth as None to display full contents\n",
    "#full_ann[['text','manual_output']].head(2000)\n",
    "\n",
    "#Reverse engineer doccano into text, creating a new column called redacted_text\n",
    "full_ann['redacted_text'] = full_ann.apply(redact_text, axis=1)\n",
    "#full_ann.head(1300) #comment in or out to examine data\n",
    "#full_ann['redacted_text'].to_csv('full_ann.csv', index=False) #This exports the redacted_text column to a .csv file for sense checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the overall analysis\n",
    "models_for_evaluation = pd.DataFrame(columns=['model_name', 'prof_details', 'path_to_results_file'], data=[\n",
    "        [\"Azure Health DeID v1\",\"with_professional_details\",\"/collaborative_development/EHR-DeID-Eval/results/2024-04-11 Microsoft HDS output.csv\"],\n",
    "        [\"anoncat\",\"with_professional_details\",\"/collaborative_development/EHR-DeID-Eval/results/2024-06-10 anoncat.csv\"],\n",
    "        #[\"anoncat\",\"without_professional_details\",\"/collaborative_development/EHR-DeID-Eval/results/2024-06-10 anoncat.csv\"],\n",
    "        [\"11 Feb 2025 OUH Fine Tuned AnonCAT 0.00002 10ep new concepts 10per\",\"with_professional_details\",\"/collaborative_development/EHR-DeID-Eval/results/2025-02-11 10per FT-Anoncat New Concepts Added 0.00002 10ep.csv\"],\n",
    "        #[\"11 Feb 2025 OUH Fine Tuned AnonCAT 0.00002 10ep new concepts 10per\",\"without_professional_details\",\"/collaborative_development/EHR-DeID-Eval/results/2025-02-11 10per FT-Anoncat New Concepts Added 0.00002 10ep.csv\"],\n",
    "        #[\"obi/deid_roberta_i2b2\",\"with_professional_details\",'/collaborative_development/EHR-DeID-Eval/results/2025-02-06 New OBI BERT benchmarks.csv'],\n",
    "        #[\"obi/deid_roberta_i2b2\",\"without_professional_details\",'/collaborative_development/EHR-DeID-Eval/results/2025-02-06 New OBI BERT benchmarks.csv'],\n",
    "        #[\"obi/deid_bert_i2b2\",\"with_professional_details\",'/collaborative_development/EHR-DeID-Eval/results/2025-02-06 New OBI BERT benchmarks.csv'],\n",
    "        #[\"obi/deid_bert_i2b2\",\"without_professional_details\",'/collaborative_development/EHR-DeID-Eval/results/2025-02-06 New OBI BERT benchmarks.csv'],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations and results for all models within the analysis\n",
    "#Make a list of unique results files for results, and pull out a list of unique files\n",
    "results_files = models_for_evaluation['path_to_results_file'].unique()\n",
    "\n",
    "#Make df to hold results\n",
    "result = pd.DataFrame()\n",
    "\n",
    "#Cycle through each unique file and load it in to the combined df\n",
    "for file in results_files:\n",
    "    new_file = pd.read_csv(file)\n",
    "    result = pd.concat([result, new_file], ignore_index = True)\n",
    "\n",
    "model_results = result.drop(columns=['Unnamed: 0', 'task_name', 'model_kwargs'])\n",
    "model_results.rename(columns={'id_number_in_dataset': 'id'}, inplace=True)\n",
    "\n",
    "#Combine datasets (doccano output and results)\n",
    "#Rename columns so that there are four: dataset (where data from), id (unique identifier), model (model output) and manual (manual annotation)\n",
    "comb1 = combine_df(model_results, full_ann, merge_criteria='id')\n",
    "#comb = comb1.drop(columns=['original_report', 'manual_output', 'dataset_y'])\n",
    "comb = comb1.copy()\n",
    "comb.rename(columns={'redacted_text': 'manual', 'model_output': 'model', 'dataset_x': 'dataset'}, inplace=True)\n",
    "\n",
    "#Convert columns to string\n",
    "comb['model'] = comb['model'].astype(str)\n",
    "comb['text'] = comb['text'].astype(str)\n",
    "comb['manual'] = comb['manual'].astype(str)\n",
    "\n",
    "#pre-process to remove any preamble\n",
    "comb['model'] = comb.apply(lambda row: update_model(row['model'], row['text']), axis=1)\n",
    "\n",
    "comb['model'] = comb['model'].apply(preprocess_text)\n",
    "comb['text'] = comb['text'].apply(preprocess_text)\n",
    "comb[['model', 'text', 'manual']] = comb[['model', 'text', 'manual']].applymap(lambda x: x.lower())\n",
    "\n",
    "#extract lists of redacted words in model and manual labelling\n",
    "comb['model_redaction'] = comb.apply(lambda row: extract_redacted_words(row, 'model'), axis=1)\n",
    "comb['manual_redaction'] = comb.apply(lambda row: extract_redacted_words(row, 'manual'), axis=1)\n",
    "\n",
    "#comb.head(1) #comment in/out to view df\n",
    "#comb[['model', 'manual']].to_csv('comb.csv', index=False) #Export for sense checking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the categories are groups of labels\n",
    "all_labels = set([label for row in comb['manual_output'] for (_, _, label) in row])\n",
    "\n",
    "categories = {\n",
    "    \"Names\":['patient name', 'hcp name'],\n",
    "    \"Other Unique Identifier\":[\"Hospital/Unit\", \"External healthcare organisation\", \"Profession\"],\n",
    "    \"Dates\":['Date', 'Age over 89'],\n",
    "    \"Medical Record Numbers\":['mrn', 'NHS number', 'specimen-identifier'],\n",
    "    \"Phone\":['Phone'], \n",
    "}\n",
    "\n",
    "#For fast results, skip Bleu calculation.\n",
    "fast_results= False\n",
    "\n",
    "#Selective redaction based on groups of labels (labels below is a list of labels)\n",
    "def redact_text_by_labels(row, labels):\n",
    "    text = preprocess_text(row['text']) #extract text from each row, using the processed text\n",
    "    manual_output = row['manual_output'] #extract manual_output for each row\n",
    "    redacted_text = text #initialise redacted text as the original text\n",
    "    offset = 0 #initialise offset to keep track fo cumulative position changes from preprocessing \n",
    "\n",
    "    #iterate over each sublist in manual_output\n",
    "    for annotation in manual_output:\n",
    "        start_token, end_token, string = annotation #extract the start, end tokens and string for each sublist\n",
    "\n",
    "        if string in labels:\n",
    "            update_start = start_token + offset\n",
    "            update_end = end_token + offset\n",
    "            redacted_text = redacted_text[:update_start] + '[' + string + ']' + redacted_text[update_end:] #replaced the text from start-end token with string and square brackets\n",
    "            len_diff = len(string) - (end_token - start_token)\n",
    "            offset += len_diff + 2 #calculate and add offset, and add +2 to account for the addition of the square brackets\n",
    "\n",
    "    return redacted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define results dfs\n",
    "# First a df for overall results\n",
    "overall_results = pd.DataFrame(columns=['model_name', 'prof_details', 'num_shots', 'precision', 'recall', 'f1', \"bleu\", \"levenshtein\"], data=[])\n",
    "\n",
    "# Next a df for recall by category\n",
    "by_category_results = pd.DataFrame(columns=['model_name', 'prof_details', 'num_shots', 'Names', 'Other Unique Identifier', 'Dates', \"Medical Record Numbers\", \"Phone\"], data=[])\n",
    "\n",
    "#Next a df for by-dataset results\n",
    "dataset_results = pd.DataFrame(columns=['model_name', 'prof_details', 'num_shots', 'dataset', 'precision', 'recall', 'f1'], data=[])\n",
    "\n",
    "#Store unique fp and fn words in a dict\n",
    "model_fp_words = {}\n",
    "model_fn_words = {}\n",
    "\n",
    "# #Remove non-applicable words as specified in Rachel's v1 code\n",
    "comb['manual_redaction'] = remove_excluded_words(comb['manual_redaction'])\n",
    "comb['model_redaction'] = remove_excluded_words(comb['model_redaction'])\n",
    "\n",
    "#Cycle through each eval config\n",
    "for index, eval in models_for_evaluation.iterrows():\n",
    "    #For that eval, select only the relevant model\n",
    "    comb_copy = comb[comb['model_name']==eval['model_name']].copy()\n",
    "\n",
    "    #Iterate through each num_shots value\n",
    "    for num_shots in comb_copy['num_shots'].unique():\n",
    "\n",
    "        #Filter out just the results for that number of shots\n",
    "        comb_copy = comb_copy[comb_copy['num_shots']==num_shots]\n",
    "\n",
    "        #Calculate the Leevenshtein distance first, prior to any editing\n",
    "        comb_copy['distance_text_model'] = comb_copy.apply(lambda row: safe_levenshtein(row['text'], row['model']), axis=1)\n",
    "        mean_ld=comb_copy['distance_text_model'].mean()\n",
    "        ld_lower_bounds, ld_upper_bounds = bootstrap_ci_levenshtein(comb_copy['distance_text_model'], num_samples=1000, ci=95)\n",
    "        ld_with_ci= (f\"{mean_ld:.3f} (95% CI: {ld_lower_bounds:.3f}, {ld_upper_bounds:.3f})\")\n",
    "        #print (ld_with_ci)\n",
    "\n",
    "        #Next, if professional details are being excluded, filter these out\n",
    "        if eval['prof_details'] == \"without_professional_details\":\n",
    "\n",
    "            # #Remove professional titles\n",
    "            comb_copy['manual_redaction'] = exclude_professional_details(comb_copy['manual_redaction'])\n",
    "            comb_copy['model_redaction'] = exclude_professional_details(comb_copy['model_redaction'])\n",
    "        \n",
    "        # #Now calculate the metrics\n",
    "        comb_copy[['tp','fp', 'fn', 'tp_words', 'fp_words', 'fn_words']] = comb_copy.apply(output_metrics, axis=1, result_type='expand')\n",
    "\n",
    "        # #Calculate the overall model results\n",
    "        grouped_comb_overall = comb_copy.groupby(['model_name','num_shots']).agg({'tp': 'sum', 'fp': 'sum', 'fn': 'sum'}).reset_index()\n",
    "        print (grouped_comb_overall.to_dict())\n",
    "\n",
    "        #Save FP and FN words\n",
    "        model_fp_words[eval['model_name']] = set(word for sublist in comb_copy.fp_words.values for word in sublist)\n",
    "        model_fn_words[eval['model_name']] = set(word for sublist in comb_copy.fn_words.values for word in sublist)\n",
    "\n",
    "        #Calculate precision, recall, f1\n",
    "        precision, recall, f1 = report_metrics_with_CIs(grouped_comb_overall.to_dict(), comb_copy)\n",
    "\n",
    "        #If producing results quickly, skip over BLEU score.\n",
    "        if not fast_results:\n",
    "            #Now calculate bleu score with CIs\n",
    "            reference = comb_copy['text'].tolist()\n",
    "            model = comb_copy['model'].tolist()\n",
    "            bleu_score_model_ref = corpus_bleu([[tokenize(ref)] for ref in reference], [tokenize(txt) for txt in model])\n",
    "            bleu_ci_model_ref_lower, bleu_ci_model_ref_upper = bootstrap_bleu(reference, model)\n",
    "            bleu_score_with_ci= (f\"{bleu_score_model_ref:.3f} (95% CI: {bleu_ci_model_ref_lower:.3f}, {bleu_ci_model_ref_upper:.3f})\")\n",
    "            print (bleu_score_with_ci)\n",
    "        else:\n",
    "            bleu_score_with_ci = \"*\"\n",
    "\n",
    "        #Write results for the overall model\n",
    "        overall_results.loc[len(overall_results)] = [eval['model_name'], eval['prof_details'], num_shots, precision, recall, f1, bleu_score_with_ci, ld_with_ci]\n",
    "    \n",
    "        #A dict to hold recalls by category results\n",
    "        by_category_results_dict={\n",
    "            'model_name': eval['model_name'],\n",
    "            'prof_details': eval['prof_details'],\n",
    "            'num_shots': num_shots\n",
    "        }\n",
    "\n",
    "        #Now calculate recall by category, where a category is a group iof labels\n",
    "        #Cycle through labels\n",
    "        for category, labels in categories.items():\n",
    "            label_df=comb_copy.copy()\n",
    "\n",
    "            print (category)\n",
    "\n",
    "            #Redact for just the one item in the reference text, and make lower case\n",
    "            label_df['manual']=label_df.apply(redact_text_by_labels, labels=labels, axis=1).str.lower() \n",
    "            label_df['manual'] = label_df['manual'].astype(str) \n",
    "\n",
    "            #extract lists of redacted words in manual labelling, with the single label. Because we are only checking one category, we cannot interpret the FP result here. \n",
    "            label_df['manual_redaction'] = label_df.apply(lambda row: extract_redacted_words(row, 'manual'), axis=1)\n",
    "            \n",
    "            #Now calculate tp, fp, etc. Make sure it is calculated only for that one label\n",
    "            label_df[['tp','fp', 'fn', 'tp_words', 'fp_words', 'fn_words']] = label_df.apply(output_metrics, axis=1, result_type='expand')\n",
    "\n",
    "            # #Calculate the overall model results; note, only the True positives are valid here.\n",
    "            grouped_comb_overall = label_df.groupby(['model_name','num_shots']).agg({'tp': 'sum', 'fp': 'sum', 'fn': 'sum'}).reset_index()\n",
    "            precision, recall, f1 = report_metrics_with_CIs(grouped_comb_overall.to_dict(), label_df)\n",
    "            print (\"tp \"+ str(label_df['tp'].sum()) + \"  fn:\" + str(label_df['fn'].sum()))\n",
    "\n",
    "            #Store recall in a dict for each category\n",
    "            by_category_results_dict[category] = recall\n",
    "\n",
    "        #Write results for the by category model\n",
    "        by_category_results.loc[len(by_category_results)] = by_category_results_dict\n",
    "        \n",
    "        #Skip dataset results if doing fast_results\n",
    "        if not fast_results:\n",
    "            #Next, calculate by-category results and add to a by_categories_df\n",
    "            #Iterate through each category\n",
    "            for dataset in comb_copy['dataset'].unique():\n",
    "                #Filter out just the results for that dataset\n",
    "                comb_dataset = comb_copy[comb_copy['dataset']==dataset]\n",
    "\n",
    "                #Calculate the dataset level results (NB: the below will return only 1 dataset as we are filtering by dataset prior to this)\n",
    "                grouped_comb_by_category = comb_dataset.groupby(['model_name', 'dataset', 'num_shots']).agg({'tp': 'sum', 'fp': 'sum', 'fn': 'sum'}).reset_index()\n",
    "\n",
    "                #Calculate the metric (NB)\n",
    "                precision, recall, f1 =  report_metrics_with_CIs(grouped_comb_by_category.to_dict(), comb_dataset, by_dataset=True)\n",
    "                dataset_results.loc[len(dataset_results)] = [eval['model_name'], eval['prof_details'], num_shots, dataset, precision, recall, f1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_category_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
